{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from supervision import Detections\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# download and load model\n",
    "model = YOLO(\"models\\\\yolo_model\\\\model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 1 FACE, 568.0ms\n",
      "Speed: 135.8ms preprocess, 568.0ms inference, 53.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x480 1 FACE, 155.5ms\n",
      "Speed: 17.1ms preprocess, 155.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 FACE, 243.1ms\n",
      "Speed: 10.5ms preprocess, 243.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 1 FACE, 245.5ms\n",
      "Speed: 5.0ms preprocess, 245.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "# Define paths for the input directory and output directory\n",
    "input_dir = \"testing_images\"  # Replace with the path to your input folder\n",
    "output_dir = \"artifacts//testing_images\"  # Replace with the path to your output folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each image in the input directory\n",
    "for image_filename in os.listdir(input_dir):\n",
    "    # Check if the file is an image (you can add more formats if needed)\n",
    "    if image_filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        image_path = os.path.join(input_dir, image_filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Load model and perform detection (assuming 'model' is defined and loaded elsewhere)\n",
    "        output = model(image)\n",
    "        detections = Detections.from_ultralytics(output[0])\n",
    "\n",
    "        # Iterate through each bounding box and save the cropped area\n",
    "        for box_id, box in enumerate(detections.xyxy):\n",
    "            x1, y1, x2, y2 = map(int, box)  # Convert coordinates to integers\n",
    "\n",
    "            # Crop the image using the bounding box coordinates\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "            # Save the cropped image with a unique filename\n",
    "            cropped_image_filename = f\"{os.path.splitext(image_filename)[0]}_box_{box_id}.png\"\n",
    "            cropped_image_path = os.path.join(output_dir, cropped_image_filename)\n",
    "            cropped_image.save(cropped_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 28 FACEs, 166.6ms\n",
      "Speed: 12.0ms preprocess, 166.6ms inference, 15.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pretrained YOLO model\n",
    "model = YOLO(\"models/yolo_model/model.pt\")  # Replace with your model path\n",
    "\n",
    "# Open the image, convert to RGB, and run inference\n",
    "image_path = \"pic.png\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "\n",
    "# Perform inference\n",
    "output = model(image)  # Run inference\n",
    "\n",
    "# Filter results to only show the face detections (class 0 is typically 'person' or 'face')\n",
    "# Output contains a list of detections with xyxy coordinates\n",
    "# You can check the class ID for face (YOLO model generally uses 0 for people/faces)\n",
    "\n",
    "# Initialize a list to store bounding boxes for faces\n",
    "face_bboxes = []\n",
    "\n",
    "for result in output[0].boxes:\n",
    "    if result.cls == 0:  # Assuming '0' is the class for faces (verify your model's class mapping)\n",
    "        x1, y1, x2, y2 = result.xyxy[0].tolist()  # Get bounding box coordinates\n",
    "        face_bboxes.append((int(x1), int(y1), int(x2), int(y2)))  # Append as integer values for drawing\n",
    "\n",
    "# Convert the image to a numpy array (OpenCV format)\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Draw the bounding boxes for faces\n",
    "for (x1, y1, x2, y2) in face_bboxes:\n",
    "    cv2.rectangle(image_np, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green bounding boxes\n",
    "\n",
    "# Convert the resulting image back to PIL for displaying\n",
    "final_image = Image.fromarray(image_np)\n",
    "\n",
    "# Display the annotated image\n",
    "final_image.show(title=\"YOLO Face Detection Result\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
