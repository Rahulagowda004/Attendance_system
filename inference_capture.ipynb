{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Attendance_system\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from supervision import Detections\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# download and load model\n",
    "model = YOLO(\"models\\\\yolo_model\\\\model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 28 FACEs, 140.6ms\n",
      "Speed: 7.0ms preprocess, 140.6ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "All bounding boxes cropped and saved in 'artifacts/inference' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Load the image and model\n",
    "image_path = \"pic.png\"\n",
    "image = Image.open(image_path)\n",
    "output = model(image)\n",
    "detections = Detections.from_ultralytics(output[0])\n",
    "\n",
    "# Create directory to save cropped images\n",
    "output_dir = \"artifacts/inference\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each bounding box and save the cropped area\n",
    "for box_id, box in enumerate(detections.xyxy):\n",
    "    x1, y1, x2, y2 = map(int, box)  # Convert coordinates to integers\n",
    "\n",
    "    # Crop the image using the bounding box coordinates\n",
    "    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    # Save the cropped image\n",
    "    cropped_image_path = os.path.join(output_dir, f\"box_{box_id}.png\")\n",
    "    cropped_image.save(cropped_image_path)\n",
    "\n",
    "print(f\"All bounding boxes cropped and saved in '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from supervision import Detections\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "\n",
    "\n",
    "class inference_cropping:\n",
    "    def __init__(self, image_path = \" provide the path\", output_directory  = \"artifacts/inference\"):\n",
    "        self.model = YOLO(\"models\\\\yolo_model\\\\model.pt\")\n",
    "        self.image_path = image_path\n",
    "        self.output_dir = output_directory\n",
    "        \n",
    "        image = Image.open(self.image_path)\n",
    "        output = self.model(image)\n",
    "        detections = Detections.from_ultralytics(output[0])\n",
    "\n",
    "        # Create directory to save cropped images\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Iterate through each bounding box and save the cropped area\n",
    "        for box_id, box in enumerate(detections.xyxy):\n",
    "            x1, y1, x2, y2 = map(int, box)  # Convert coordinates to integers\n",
    "\n",
    "            # Crop the image using the bounding box coordinates\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            # Save the cropped image\n",
    "            cropped_image_path = os.path.join(output_dir, f\"box_{box_id}.png\")\n",
    "            cropped_image.save(cropped_image_path)\n",
    "\n",
    "        print(f\"All bounding boxes cropped and saved in '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 28 FACEs, 77.6ms\n",
      "Speed: 3.0ms preprocess, 77.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "All bounding boxes cropped and saved in 'artifacts/inference' directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.inference_cropping at 0x2552cd15c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_cropping(image_path = \"pic.png\", output_directory = \"artifacts/inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
