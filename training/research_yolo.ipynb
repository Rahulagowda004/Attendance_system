{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from supervision import Detections\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# download and load model\n",
    "model = YOLO(\"models/yolo_model/model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 608x640 29 FACEs, 257.4ms\n",
      "Speed: 9.0ms preprocess, 257.4ms inference, 5.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "bound box = [[     259.41      308.27      291.97      342.43]\n",
      " [     790.16      307.21      825.19      348.24]\n",
      " [      453.5      328.56      481.96      364.92]\n",
      " [     399.95      314.73      431.59      352.02]\n",
      " [     61.833      347.77      93.337      384.23]\n",
      " [     680.46       343.1       713.2      377.78]\n",
      " [     497.69      335.04      527.55      370.52]\n",
      " [     565.13      461.96       595.7      496.66]\n",
      " [     124.37      327.43       155.3      363.51]\n",
      " [     304.06      324.31      332.47      356.55]\n",
      " [     20.009      332.62       47.02      371.12]\n",
      " [     647.18      335.49      675.49      372.27]\n",
      " [     737.05      346.62      768.44      381.41]\n",
      " [     613.18      305.45      640.09      340.72]\n",
      " [     561.44      328.86      590.18      364.48]\n",
      " [      342.1      330.22      370.16      364.99]\n",
      " [     469.81      479.59      503.15      514.47]\n",
      " [     421.88      469.71      451.35      502.45]\n",
      " [     184.54       335.9      220.19      364.66]\n",
      " [     287.17       478.6      314.94      510.39]\n",
      " [     72.407      478.44      107.18      514.53]\n",
      " [      366.9      310.07      393.16      339.91]\n",
      " [     199.17       497.2      232.15      529.13]\n",
      " [     631.09      470.67      667.97      503.54]\n",
      " [     236.03      343.47      264.36      367.86]\n",
      " [     339.51       487.8      368.11      515.11]\n",
      " [     709.73      506.29      738.03      535.33]\n",
      " [     192.59      334.71      223.54      364.01]\n",
      " [       8.73      295.85      24.684      321.75]]\n"
     ]
    }
   ],
   "source": [
    "image_path = \"training/pic1.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# run inference\n",
    "output = model(image)\n",
    "detections = Detections.from_ultralytics(output[0])\n",
    "\n",
    "print( f\"bound box = {detections.xyxy}\")\n",
    "\n",
    "# draw bounding boxes\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in detections.xyxy:  # corrected to detections.xyxy\n",
    "    # box coordinates\n",
    "    x1, y1, x2, y2 = box\n",
    "    draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://yolov8.org/how-to-use-fine-tune-yolov8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pre-trained YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # Use yolov8n.pt, yolov8s.pt, etc. depending on the size\n",
    "\n",
    "# Train the model on your dataset\n",
    "model.train(\n",
    "    data=\"data.yaml\",  # Path to your data.yaml file\n",
    "    epochs=50,         # Number of training epochs\n",
    "    imgsz=640,         # Image size\n",
    "    batch=16,          # Batch size\n",
    "    name=\"yolov8_face_finetune\",  # Experiment name\n",
    "    pretrained=True    # Start from the pre-trained weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade model for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the image\n",
    "# Use double backslashes or a raw string to specify the path correctly\n",
    "img = cv2.imread(r'research\\pic1.png')  # or use 'research\\\\pic1.png'\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Draw rectangles around each face\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Detected Faces', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to start capturing 50 images for class 'rahul'...\n",
      "Started capturing for class 'rahul'\n",
      "Captured image 1 for class 'rahul'\n",
      "Captured image 2 for class 'rahul'\n",
      "Captured image 3 for class 'rahul'\n",
      "Captured image 4 for class 'rahul'\n",
      "Captured image 5 for class 'rahul'\n",
      "Captured image 6 for class 'rahul'\n",
      "Captured image 7 for class 'rahul'\n",
      "Captured image 8 for class 'rahul'\n",
      "Captured image 9 for class 'rahul'\n",
      "Captured image 10 for class 'rahul'\n",
      "Captured image 11 for class 'rahul'\n",
      "Captured image 12 for class 'rahul'\n",
      "Captured image 13 for class 'rahul'\n",
      "Captured image 14 for class 'rahul'\n",
      "Captured image 15 for class 'rahul'\n",
      "Captured image 16 for class 'rahul'\n",
      "Captured image 17 for class 'rahul'\n",
      "Captured image 18 for class 'rahul'\n",
      "Captured image 19 for class 'rahul'\n",
      "Captured image 20 for class 'rahul'\n",
      "Captured image 21 for class 'rahul'\n",
      "Captured image 22 for class 'rahul'\n",
      "Captured image 23 for class 'rahul'\n",
      "Captured image 24 for class 'rahul'\n",
      "Captured image 25 for class 'rahul'\n",
      "Captured image 26 for class 'rahul'\n",
      "Captured image 27 for class 'rahul'\n",
      "Captured image 28 for class 'rahul'\n",
      "Captured image 29 for class 'rahul'\n",
      "Captured image 30 for class 'rahul'\n",
      "Captured image 31 for class 'rahul'\n",
      "Captured image 32 for class 'rahul'\n",
      "Captured image 33 for class 'rahul'\n",
      "Captured image 34 for class 'rahul'\n",
      "Captured image 35 for class 'rahul'\n",
      "Captured image 36 for class 'rahul'\n",
      "Captured image 37 for class 'rahul'\n",
      "Captured image 38 for class 'rahul'\n",
      "Captured image 39 for class 'rahul'\n",
      "Captured image 40 for class 'rahul'\n",
      "Captured image 41 for class 'rahul'\n",
      "Captured image 42 for class 'rahul'\n",
      "Captured image 43 for class 'rahul'\n",
      "Captured image 44 for class 'rahul'\n",
      "Captured image 45 for class 'rahul'\n",
      "Captured image 46 for class 'rahul'\n",
      "Captured image 47 for class 'rahul'\n",
      "Started capturing for class 'rahul'\n",
      "Captured image 48 for class 'rahul'\n",
      "Captured image 49 for class 'rahul'\n",
      "Captured image 50 for class 'rahul'\n",
      "Finished capturing for class 'rahul'\n",
      "Press 's' to start capturing 50 images for class 'rahul1'...\n",
      "Finished capturing for class 'rahul1'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "num_classes = 2\n",
    "num_images = 50\n",
    "output_dir = \"captured_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV file for bounding box annotations\n",
    "annotations_file = os.path.join(output_dir, \"annotations.csv\")\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(annotations_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header for CSV\n",
    "    writer.writerow([\"image\", \"class\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "\n",
    "    # Loop through each class\n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = input(f\"Enter name for class {class_idx + 1}: \")\n",
    "        print(f\"Press 's' to start capturing {num_images} images for class '{class_name}'...\")\n",
    "\n",
    "        captured_count = 0  # Counter for images in the current class\n",
    "        start_capturing = False\n",
    "\n",
    "        while captured_count < num_images:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert frame to grayscale (required for face detection)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the frame\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            # Draw bounding boxes around detected faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "                # If capturing is started, save the image and record the bounding box\n",
    "                if start_capturing and captured_count < num_images:\n",
    "                    # Define bounding box coordinates\n",
    "                    xmin, ymin, xmax, ymax = x, y, x + w, y + h\n",
    "                    img_filename = f\"{output_dir}/frame_class_{class_name}_{captured_count}.jpg\"\n",
    "\n",
    "                    # Save the image with bounding box\n",
    "                    cv2.imwrite(img_filename, frame)\n",
    "                    \n",
    "                    # Save annotation to CSV\n",
    "                    writer.writerow([img_filename, class_name, xmin, ymin, xmax, ymax])\n",
    "\n",
    "                    captured_count += 1\n",
    "                    print(f\"Captured image {captured_count} for class '{class_name}'\")\n",
    "\n",
    "            # Display the frame with bounding boxes\n",
    "            cv2.imshow('Face Detection', frame)\n",
    "\n",
    "            # Start capturing when \"s\" is pressed\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('s'):\n",
    "                start_capturing = True\n",
    "                print(f\"Started capturing for class '{class_name}'\")\n",
    "            elif key == ord('q'):\n",
    "                # Exit the program if \"q\" is pressed\n",
    "                break\n",
    "\n",
    "        # Reset start_capturing for the next class\n",
    "        start_capturing = False\n",
    "        print(f\"Finished capturing for class '{class_name}'\")\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
